defaults:
  - base_gpn
  - optimizer: adam
  - optimizer/adam@warmup_optimizer
  - evaluation: default

optimizer:
  weight_decay: 1e-3
  lr: 1e-3
warmup_optimizer:
  weight_decay: ${model.trainer.flow_weight_decay}
  lr: ${model.trainer.flow_lr}

flow_lr: 1e-2
flow_weight_decay: 0
entropy_regularization_loss_weight: 1e-4 # regularization strength for the entropy of the posterior
num_warmup_epochs: 5

warmup: FLOW

max_epochs: 10000
commit_to_wandb_every_epoch: ~ # only commit in the end

early_stopping:
  monitor:
    name: LOSS
    dataset_split: VAL
  higher_is_better: false
  patience: 50
  min_delta: 1e-3
  save_model_state: true
